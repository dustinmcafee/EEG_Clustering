{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform K-Means++. See file kmeans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\"\"\"\n",
    "Name: Dustin Mcafee\n",
    "Class: COSC 594 - Big Data Analysis\n",
    "Perform K-Means++ on dataset\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import genfromtxt\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "\n",
    "# Add value1 and value 2\n",
    "# Useful as a reduce function\n",
    "def addValues(val1, val2):\n",
    "    return val1 + val2\n",
    "\n",
    "# Find the centroid that the point is closest to and return the centroid's index\n",
    "# Input: point: point array(x, y)\n",
    "#        centroids: array[array(x1,y1), array(x2,y2), ..., array(xk,yk)], \n",
    "#                   where k is the number of clusters\n",
    "# Return: closest centroid index\n",
    "def getClosestCentroid(point, centroids):\n",
    "    distances = [np.sqrt(sum((point - centroid)**2)) for centroid in centroids]\n",
    "    return np.argmin(distances)\n",
    "\n",
    "# Given a point and a list of centroids, assign point to cluster of closest centroid\n",
    "# Input: points_rdd: PipelinedRDD <<array(x1,y1), array(x2,y2), ... array(xN,yN)>>,\n",
    "#                    where N is the number of lines in the file\n",
    "#        centroids:  array[array(x1,y1), array(x2,y2), ..., array(xk,yk)],\n",
    "#                    where k is the number of clusters\n",
    "# Return: RDD of clustered points: <<(clusterID, np.array(x1, y1)), (clusterID, np.array(x2, y2)), ...>>\n",
    "def assignPointsToClosestCluster(points_rdd, centroids):\n",
    "    return points_rdd.map(lambda x: (getClosestCentroid(x, centroids), x))\n",
    "    \n",
    "# Sum the euclidean distance of two points\n",
    "# Input: points1: array[array(x1,y1), array(x2,y2), ..., array(xk,yk)],\n",
    "#        points2: array[array(x1,y1), array(x2,y2), ..., array(xk,yk)],\n",
    "# Return: sum of distances\n",
    "def sumDistances(points1, points2):\n",
    "    return sum([np.sqrt(sum((one - two)**2)) for one, two in zip(points1, points2)])\n",
    "\n",
    "# Calculate the mean coordinates of each cluster.\n",
    "# Input: clustered_points_rdd: <<clustered_point1, clustered_point2, ..., clustered_pointN>>,\n",
    "#                              where N is the number of clustered_points, and\n",
    "#                              each clustered_point looks like (clusterID, array(x,y))\n",
    "# Return: cluster means [centroid1, centroid2, ..., centroidK],\n",
    "#         where K is the number of clusters, and\n",
    "#         where each centroid is array(x,y)\n",
    "def calculateClusterMeans(clustered_points_rdd):\n",
    "    # Sum the xs and ys of all points in each cluster\n",
    "    sum_points = clustered_points_rdd.reduceByKey(addValues)\n",
    "    \n",
    "    # Count the number of points in each cluster\n",
    "    counts = clustered_points_rdd.countByKey()\n",
    "    \n",
    "    # Divide the x,y sums for each cluster by the number of points in each cluster\n",
    "    cluster_means = []\n",
    "    for key, value in sum_points.collect():\n",
    "        n = counts.get(key)\n",
    "        avg = value / n\n",
    "        cluster_means.append(avg)\n",
    "    \n",
    "    return cluster_means\n",
    "\n",
    "#Calculate Distance Matrix and calculate centroids using\n",
    "# weighted probability proportional to the distances squared\n",
    "# (i.e. kmeans++ method)\n",
    "# Input: data: data matrix that contains the points (one per row)\n",
    "#        K: number of clusters\n",
    "# Return: [centroid1, centroid2, ..., centroidK]\n",
    "def centroid(data, K):\n",
    "    # Initialize Array Variables\n",
    "    centroids = []\n",
    "    used_indices = []\n",
    "    # Create distance matrix\n",
    "    dist_matrix = distance(data)\n",
    "    indices = np.arange(np.size(data, 0))\n",
    "    # Choose first index randomly\n",
    "    index = np.random.choice(indices, 1)\n",
    "    used_indices.append(index)\n",
    "    for j in range(0, K):\n",
    "        centroids.append(data[index][0])\n",
    "        d = np.full((1, np.size(data, 0)), np.inf)\n",
    "        # For each data point x, compute the distance between x and the nearest centroid that has already been chosen.\n",
    "        for i in used_indices:\n",
    "            dist = dist_matrix[i]\n",
    "            d = np.concatenate((d, dist))\n",
    "        # Create probability distrubution from minimum distances\n",
    "        min_prob = np.amin(d, axis=0)\n",
    "        prob = np.square(min_prob)\n",
    "        prob = min_prob / np.sum(min_prob)\n",
    "        # Choose centroids by probability proportional to distance of nearest centroid\n",
    "        index = np.random.choice(indices, 1, p=prob)\n",
    "        used_indices.append(index)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "# Cluster the points in the data matrix into K clusters using k-means clustering\n",
    "# Input: data: data matrix that contains the points (point per row)\n",
    "#        K: number of clusters\n",
    "# Return: number of iterations, [centroid1, centroid2, ..., centroidK] and [clustered_point1, clustered_point2, ..., clustered_pointN]\n",
    "#         where K is the number of clusters,\n",
    "#         where N is the number of points,\n",
    "#         where centroid is np.array(x,y), and\n",
    "#         where each clustered_point is (clusterID, np.array(x,y))\n",
    "def clusterData(data, K):\n",
    "    # Choose K points as the centroids using K-means++\n",
    "    centroids = centroid(data, K)\n",
    "\n",
    "    # Create RDD from data\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    points = sc.parallelize(data, numSlices=500)\n",
    "\n",
    "    # Assign each point to the centroid closest to it\n",
    "    clustered_points = assignPointsToClosestCluster(points, centroids)\n",
    "    \n",
    "    # Begin the iterative portion of k-means,\n",
    "    # continue until the changes in centroids are very small (e.g., < .0001)\n",
    "    change_in_centroids = np.inf\n",
    "    old_change = 0\n",
    "    iterations = 0\n",
    "    while change_in_centroids > 0.005:\n",
    "        old_centroids = centroids\n",
    "        # Calculate the new centroids based on the means of the current clusters\n",
    "        centroids = calculateClusterMeans(clustered_points)\n",
    "        \n",
    "        # Assign the points to the new centroids\n",
    "        clustered_points = assignPointsToClosestCluster(points, centroids)\n",
    "        \n",
    "        # Calculate the change in the centroids since the last iteration\n",
    "        change_in_centroids = sumDistances(old_centroids, centroids)\n",
    "        if(change_in_centroids == old_change):\n",
    "            break\n",
    "        old_change = change_in_centroids\n",
    "        iterations = iterations + 1\n",
    "        print(\"Iteration:\", iterations, \"Change:\", change_in_centroids)\n",
    "\n",
    "    return iterations, centroids, clustered_points.collect()\n",
    "\n",
    "# Compute the distance matrix given a (normalized) data matrix\n",
    "# Input: data: (normalized) N x D data matrix\n",
    "# Return: N x N symmetric Distance Matrix\n",
    "def distance(data):\n",
    "    DF_var = pd.DataFrame(data)\n",
    "    distances = squareform(pdist(DF_var, metric='euclidean'))\n",
    "    return distances\n",
    "    \n",
    "# Helper function for getPerformanceMetrics.\n",
    "# Computes number of correct/incorrect predictions,\n",
    "# as well as true positives/negatives, and false positives/negatives\n",
    "# Input: validData: the valid list of categories\n",
    "#\t predictions: the predicted list of categories\n",
    "# Return: number of correct, incorrect, true positives,\n",
    "#\t  true negatives, false positives, and false negatives\n",
    "def getPerformanceStatistics(validData, predictions):\n",
    "\tcorrect = 0\n",
    "\tincorrect = 0\n",
    "\tfNeg = 0\n",
    "\tfPos = 0\n",
    "\ttPos = 0\n",
    "\ttNeg = 0\n",
    "\tfor x in range(len(validData)):\n",
    "\t\tif(validData[x] == predictions[x]):\n",
    "\t\t\tcorrect += 1\n",
    "\t\t\tif(validData[x] == 0):\t\t\t\t#True negative\n",
    "\t\t\t\ttNeg +=1\n",
    "\t\t\telse:\t\t\t\t\t\t#True positive\n",
    "\t\t\t\ttPos +=1\n",
    "\t\telse:\n",
    "\t\t\tincorrect += 1\n",
    "\t\t\t# 1 == seizure (positive), 0 == no seizure (negative)\n",
    "\t\t\tif(validData[x] == 1):\t\t\t\t#False negative\n",
    "\t\t\t\tfNeg +=1\n",
    "\t\t\telse:\t\t\t\t\t\t#False positive\n",
    "\t\t\t\tfPos +=1\n",
    "\treturn correct, incorrect, tPos, tNeg, fPos, fNeg\n",
    "\n",
    "#Report performance metrics from the validation dataset and list of predictions\n",
    "#Returns tuple\n",
    "def getPerformanceMetrics(validData, predictions):\n",
    "\tP, N, TP, TN, FP, FN = getPerformanceStatistics(validData, predictions)\n",
    "\taccuracy = (P / (P + N)) * 100.0\n",
    "\tif((TP + FN) == 0):\n",
    "\t\tTPR = np.inf\n",
    "\telse:\n",
    "\t\tTPR = TP / (TP + FN) * 100.0\t#True positive rate; recall; sensitivity\n",
    "\tif((TP + FP) == 0):\n",
    "\t\tPPV = np.inf\n",
    "\telse:\n",
    "\t\tPPV = TP / (TP + FP) * 100.0\t#Positive predictive value; precision\n",
    "\tif((TN + FP) == 0):\n",
    "\t\tTNR = np.inf\n",
    "\telse:\n",
    "\t\tTNR = TN / (TN + FP) * 100.0\t#True negative rate; specificity\n",
    "\tif((PPV + TPR) == 0):\n",
    "\t\tF = np.inf\n",
    "\telse:\n",
    "\t\tF = 2 * PPV * TPR / (PPV + TPR)\t#F1 Score\n",
    "\treturn (P, N, TP, TN, FP, FN, accuracy, TPR, PPV, TNR, F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing K-Means on Training Dataset for 2 clusters\n",
      "Iteration: 1 Change: 13.037724983801887\n",
      "Iteration: 2 Change: 5.824587552685009\n",
      "Iteration: 3 Change: 2.677357750618371\n",
      "Iteration: 4 Change: 1.9120994479487246\n",
      "Iteration: 5 Change: 1.439618568169003\n",
      "Iteration: 6 Change: 1.2016086497693066\n",
      "Iteration: 7 Change: 0.6917834203505082\n",
      "Iteration: 8 Change: 0.5316174474388851\n",
      "Iteration: 9 Change: 0.6355809804838032\n",
      "Iteration: 10 Change: 0.6392499211841056\n",
      "Iteration: 11 Change: 0.6336092739656273\n",
      "Iteration: 12 Change: 0.5981417363317301\n",
      "Iteration: 13 Change: 0.3226952589668782\n",
      "Iteration: 14 Change: 0.21360460850468105\n",
      "Iteration: 15 Change: 0.3437648690866555\n",
      "Iteration: 16 Change: 0.14112234807242582\n",
      "Iteration: 17 Change: 0.4741048102429755\n",
      "Iteration: 18 Change: 0.47585092227573056\n",
      "Iteration: 19 Change: 0.40661354080113116\n",
      "Iteration: 20 Change: 0.5046706765281355\n",
      "Iteration: 21 Change: 0.6304276885596632\n",
      "Iteration: 22 Change: 0.4419145629729112\n",
      "Iteration: 23 Change: 0.43830441353172067\n",
      "Iteration: 24 Change: 0.3113580985436874\n",
      "Iteration: 25 Change: 0.24443428193949526\n",
      "Iteration: 26 Change: 0.2640634551270401\n",
      "Iteration: 27 Change: 0.08417447737394149\n",
      "Iteration: 28 Change: 0.0\n",
      "Kmeans Converged after 28 iterations\n",
      "Minimum Intercluster Distance: 7.955739314482344\n",
      "Maximum Intracluster Distance: 91.70647930904336\n",
      "Dunn index (Min Intercluster Distance / Max Intracluster Dist): 0.08675220523592614\n",
      "Training Set Performance Metrics:\n",
      "Accuracy: 83.56%\n",
      "True Positives: 290\n",
      "True Negatives: 7063\n",
      "False Positives: 4\n",
      "False Negatives: 1443\n",
      "True Positive Rate (sensitivity): 16.73%\n",
      "Positive Prediction Value (precision): 98.64%\n",
      "True Negative Rate (specificity): 99.94%\n",
      "F1 Score: 28.61%\n"
     ]
    }
   ],
   "source": [
    "    #Command Line Argument\n",
    "    k = 2\n",
    "    if k <= 0:\n",
    "        print(\"First arguement must be > 0\")\n",
    "        sys.quit()\n",
    "    print(\"Performing K-Means on Training Dataset for\", k, \"clusters\")\n",
    "\n",
    "    #Load data matrix\n",
    "    my_data = genfromtxt(\"input/train/TrainingData_Projected.txt\", delimiter=',')\n",
    "    testing_data = genfromtxt(\"input/test/TestingData_Projected.txt\", delimiter=',')\n",
    "\n",
    "    #Delete first row: header\n",
    "    my_data = np.delete(my_data, 0, 0)\n",
    "    #Delete first column: ID\n",
    "    my_data = np.delete(my_data, 0, 1)\n",
    "    #Collect labels and delete label column (last column)\n",
    "    dimensions = np.size(my_data, 1)\n",
    "    labels = my_data[:,-1].copy()\n",
    "    my_data = np.delete(my_data, -1, 1)\n",
    "\n",
    "    #Initialize Variables\n",
    "    dimensions = np.size(my_data, 1)\n",
    "    N = np.size(my_data, 0)\n",
    "    categories = np.zeros((N,1), dtype=int)\n",
    "    arr = np.array([])\n",
    "    for i in range(N):\n",
    "        arr = np.append(arr, i)\n",
    "    arr = arr.reshape(N,1)\n",
    "\n",
    "    #Distance Matrix\n",
    "    dist_matrix = distance(my_data)\n",
    "\n",
    "    #K-means Clustering\n",
    "    iterations, centroids, clustered_points = clusterData(my_data, k)\n",
    "    print(\"Kmeans Converged after\", iterations, \"iterations\")\n",
    "    i = 0\n",
    "\n",
    "    #Prepend Categories for Plotting the Data\n",
    "    for elem in clustered_points:\n",
    "        categories[i][0] = elem[0]\n",
    "        i = i + 1\n",
    "    my_data = np.hstack((categories, my_data))\n",
    "\n",
    "    #Minimal Intercluster Distance\n",
    "    min_inter = np.inf\n",
    "    for i in range(0, N):\n",
    "        elem = my_data[i]\n",
    "        for j in range(i+1, N):\n",
    "            if(not my_data[j][0] == elem[0]):\n",
    "                if(dist_matrix[i][j] < min_inter):\n",
    "                    min_inter = dist_matrix[i][j]\n",
    "    print(\"Minimum Intercluster Distance:\", min_inter)\n",
    "\n",
    "    #Maximal Intracluster Distance\n",
    "    max_intra = -np.inf\n",
    "    for i in range(0, N):\n",
    "        elem = my_data[i]\n",
    "        for j in range(i+1, N):\n",
    "            if(my_data[j][0] == elem[0]):\n",
    "                if(dist_matrix[i][j] > max_intra):\n",
    "                    max_intra = dist_matrix[i][j]\n",
    "    print(\"Maximum Intracluster Distance:\", max_intra)\n",
    "\n",
    "    #Dunn Index\n",
    "    dunn = min_inter / max_intra\n",
    "    print(\"Dunn index (Min Intercluster Distance / Max Intracluster Dist):\", dunn)\n",
    "\n",
    "    # Print metrics\n",
    "    P, N, TP, TN, FP, FN, accuracy, TPR, PPV, TNR, F = getPerformanceMetrics(labels, my_data[:,0])\n",
    "    print('Training Set Performance Metrics:')\n",
    "    print('Accuracy: ' + \"{0:.2f}\".format(round(accuracy,2)) + '%')\n",
    "    print('True Positives: ' + str(TP))\n",
    "    print('True Negatives: ' + str(TN))\n",
    "    print('False Positives: ' + str(FP))\n",
    "    print('False Negatives: ' + str(FN))\n",
    "    print('True Positive Rate (sensitivity): ' + \"{0:.2f}\".format(round(TPR,2)) + '%')\n",
    "    print('Positive Prediction Value (precision): ' + \"{0:.2f}\".format(round(PPV,2)) + '%')\n",
    "    print('True Negative Rate (specificity): ' + \"{0:.2f}\".format(round(TNR,2)) + '%')\n",
    "    print('F1 Score: ' + \"{0:.2f}\".format(round(F,2)) + '%')\n",
    "\n",
    "    #Save output Files\n",
    "    filename = \"output/Data_Kmeans_\" + str(k) + \".txt\"\n",
    "    np.savetxt(filename, my_data, delimiter=',', fmt='%3.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing K-Means on Testing Dataset for 2 clusters\n",
      "Minimum Intercluster Distance: 1.2446314675437062\n",
      "Maximum Intracluster Distance: 89.69229342752921\n",
      "Dunn index (Min Intercluster Distance / Max Intracluster Dist): 0.013876682376834978\n",
      "Testing Set Performance Metrics:\n",
      "Accuracy: 82.22%\n",
      "True Positives: 87\n",
      "True Negatives: 2132\n",
      "False Positives: 2\n",
      "False Negatives: 478\n",
      "True Positive Rate (sensitivity): 15.40%\n",
      "Positive Prediction Value (precision): 97.75%\n",
      "True Negative Rate (specificity): 99.91%\n",
      "F1 Score: 26.61%\n"
     ]
    }
   ],
   "source": [
    "    print(\"Performing K-Means on Testing Dataset for\", k, \"clusters\")\n",
    "\n",
    "    #Delete first row: header\n",
    "    testing_data = np.delete(testing_data, 0, 0)\n",
    "    #Delete first column: ID\n",
    "    testing_data = np.delete(testing_data, 0, 1)\n",
    "    #Collect labels and delete label column (last column)\n",
    "    test_labels = testing_data[:,-1].copy()\n",
    "    testing_data = np.delete(testing_data, -1, 1)\n",
    "\n",
    "    # Create RDD from data\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    test_points = sc.parallelize(testing_data, numSlices=50)\n",
    "\n",
    "    # Assign each point to the centroid closest to it\n",
    "    test_clusters = assignPointsToClosestCluster(test_points, centroids).collect()\n",
    "\n",
    "    #Prepend Categories for Plotting the Data\n",
    "    N = np.size(testing_data, 0)\n",
    "    cat = np.zeros((N,1), dtype=int)\n",
    "    i = 0\n",
    "    for elem in test_clusters:\n",
    "        cat[i][0] = elem[0]\n",
    "        i = i + 1\n",
    "    testing_data = np.hstack((cat, testing_data))\n",
    "\n",
    "    #Minimal Intercluster Distance\n",
    "    min_inter = np.inf\n",
    "    for i in range(0, N):\n",
    "        elem = testing_data[i]\n",
    "        for j in range(i+1, N):\n",
    "            if(not testing_data[j][0] == elem[0]):\n",
    "                if(dist_matrix[i][j] < min_inter):\n",
    "                    min_inter = dist_matrix[i][j]\n",
    "    print(\"Minimum Intercluster Distance:\", min_inter)\n",
    "\n",
    "    #Maximal Intracluster Distance\n",
    "    max_intra = -np.inf\n",
    "    for i in range(0, N):\n",
    "        elem = testing_data[i]\n",
    "        for j in range(i+1, N):\n",
    "            if(testing_data[j][0] == elem[0]):\n",
    "                if(dist_matrix[i][j] > max_intra):\n",
    "                    max_intra = dist_matrix[i][j]\n",
    "    print(\"Maximum Intracluster Distance:\", max_intra)\n",
    "\n",
    "    #Dunn Index\n",
    "    dunn = min_inter / max_intra\n",
    "    print(\"Dunn index (Min Intercluster Distance / Max Intracluster Dist):\", dunn)\n",
    "\n",
    "    # Print metrics\n",
    "    P, N, TP, TN, FP, FN, accuracy, TPR, PPV, TNR, F = getPerformanceMetrics(test_labels, testing_data[:,0])\n",
    "    print('Testing Set Performance Metrics:')\n",
    "    print('Accuracy: ' + \"{0:.2f}\".format(round(accuracy,2)) + '%')\n",
    "    print('True Positives: ' + str(TP))\n",
    "    print('True Negatives: ' + str(TN))\n",
    "    print('False Positives: ' + str(FP))\n",
    "    print('False Negatives: ' + str(FN))\n",
    "    print('True Positive Rate (sensitivity): ' + \"{0:.2f}\".format(round(TPR,2)) + '%')\n",
    "    print('Positive Prediction Value (precision): ' + \"{0:.2f}\".format(round(PPV,2)) + '%')\n",
    "    print('True Negative Rate (specificity): ' + \"{0:.2f}\".format(round(TNR,2)) + '%')\n",
    "    print('F1 Score: ' + \"{0:.2f}\".format(round(F,2)) + '%')\n",
    "\n",
    "    #Save output Files\n",
    "    filename = \"output/TestingData_Kmeans_\" + str(k) + \".txt\"\n",
    "    np.savetxt(filename, testing_data, delimiter=',', fmt='%3.4f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
