{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization. See file exmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\"\"\"\n",
    "Name: Dustin Mcafee\n",
    "Perform Expectation Maximization Assuming K Gaussian Clusters\n",
    "\"\"\"\n",
    "from pyspark import SparkContext\t\t# For Spark mapreduce\n",
    "import sys\t\t\t\t\t# For system arguments\n",
    "import numpy as np\t\t\t\t# For fast matrix multiplication and linear algrebra\n",
    "from numpy import genfromtxt\t\t\t# For reading csv\n",
    "from scipy.stats import multivariate_normal\t# For multivariate normal probability distribution\n",
    "from scipy.misc import logsumexp\t\t# For use with map to sum likelihoods\n",
    "from sklearn.metrics import silhouette_score\t# For silhoutte score\n",
    "import pandas as pd\t\t\t\t# For fast computation of distance matrix\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "\n",
    "# Find the centroid that the `point` is closest to and return the centroid's ID\n",
    "# The centroid ID in this case is simply its index in the `centroids` list\n",
    "# Input: point: np.array(x,y)\n",
    "#        centroids: [np.array(x1,y1), np.array(x2,y2), ..., np.array(xK,yK)], \n",
    "#                   where K is the number of clusters\n",
    "# Return: clusterID\n",
    "def getClosestCentroidID(point, centroids):\n",
    "    distances = [np.sqrt(sum((point - centroid)**2)) for centroid in centroids]\n",
    "    return np.argmin(distances)\n",
    "\n",
    "# Given a point (i.e., (x,y) and a list of centroids (i.e., list of points),\n",
    "# find the closest centroid and assign that cluster to the point\n",
    "# Input: points_rdd: <<np.array(x1,y1), np.array(x2,y2), ... np.array(xN,yN)>>,\n",
    "#                    where N is the number of lines in the file\n",
    "#        centroids:  [np.array(x1,y1), np.array(x2,y2), ..., np.array(xK,yK)],\n",
    "#                    where K is the number of clusters\n",
    "# Return: RDD of clustered points: <<(clusterID, np.array(x1, y1)), (clusterID, np.array(x2, y2)), ...>>\n",
    "def assignPointsToClosestCluster(points_rdd, centroids):\n",
    "    return points_rdd.map(lambda x: (getClosestCentroidID(x, centroids), x))\n",
    "    \n",
    "# Sum the distance that each centroid moved by\n",
    "# Input: old_centroids: [np.array(x1,y1), np.array(x2,y2), ..., np.array(xK,yK)],\n",
    "#                       where K is the number of clusters\n",
    "#        new_centroids: [np.array(x1,y1), np.array(x2,y2), ..., np.array(xK,yK)],\n",
    "#                       where K is the number of clusters\n",
    "# Return: sum of distances\n",
    "def calculateChangeInCentroids(old_centroids, new_centroids):\n",
    "    a = np.array(old_centroids, dtype=float)\n",
    "    b = np.array(new_centroids, dtype=float)\n",
    "    return sum([np.sqrt(sum((old - new)**2)) for old, new in zip(a, b)])\n",
    "\n",
    "# Compute the distance matrix given a (normalized) data matrix\n",
    "# Input: data: (normalized) N x D data matrix\n",
    "# Return: distances: N x N symmetric Distance Matrix\n",
    "def distance(data):\n",
    "    DF_var = pd.DataFrame(data)\n",
    "    distances = squareform(pdist(DF_var, metric='euclidean'))\n",
    "    return distances\n",
    "\n",
    "#Calculate Distance Matrix and calculate centroids using\n",
    "# weighted probability proportional to the distances squared\n",
    "# (i.e. kmeans++ method)\n",
    "# Input: data: data matrix that contains the points (one per row)\n",
    "#        K: number of clusters\n",
    "# Return: centoids: [centroid1, centroid2, ..., centroidK]\n",
    "#         dist_matrix: distance matrix of data points\n",
    "def centroid(data, K):\n",
    "    # Initialize Array Variables\n",
    "    centroids = []\n",
    "    used_indices = []\n",
    "\n",
    "    # Create distance matrix\n",
    "    dist_matrix = distance(data)\n",
    "    indices = np.arange(np.size(data, 0))\n",
    "\n",
    "    # Choose first index randomly\n",
    "    index = np.random.choice(indices, 1)\n",
    "    used_indices.append(index)\n",
    "\n",
    "    # Find probable initial centroids\n",
    "    for j in range(0, K):\n",
    "        centroids.append(data[index][0])\n",
    "        d = np.full((1, np.size(data, 0)), np.inf)\n",
    "\n",
    "        # For each data point x, compute the distance between x and the nearest centroid that has already been chosen.\n",
    "        for i in used_indices:\n",
    "            dist = dist_matrix[i].copy()\n",
    "            d = np.concatenate((d, dist))\n",
    "\n",
    "        # Create probability distrubution from minimum distances\n",
    "        min_prob = np.amin(d, axis=0)\n",
    "        prob = np.square(min_prob)\n",
    "        prob = min_prob / np.sum(min_prob)\n",
    "\n",
    "        # Choose centroids by probability proportional to distance of nearest centroid\n",
    "        index = np.random.choice(indices, 1, p=prob)\n",
    "        used_indices.append(index)\n",
    "\n",
    "    return centroids, dist_matrix\n",
    "\n",
    "# Compute the Log PDF (Multivariate Normal) (i.e. the Log Likelihood)\n",
    "# for one row of the data (the Kth row)\n",
    "# Input: data: data matrix (N x D 2d list)\n",
    "#\t k: number of clusters\n",
    "#\t prob_mat: Pior Probabilities (N x k 2d list)\n",
    "#\t means: mean of each cluster (list)\n",
    "#\t covs: covariance of each cluster (list)\n",
    "# Return: list of log likelihoods of x belonging to class k\n",
    "def Log_Likelihood(data, k, prob_mat, means, covs):\n",
    "\treturn [(np.log(prob_mat[i]) + multivariate_normal.logpdf(data[0],mean=means[i],cov=covs[i],allow_singular=True)) for i in range(k)]\n",
    "\n",
    "# Map function to compute updated likelihood values\n",
    "# Input: x: data row\n",
    "#\t k: number of clusters\n",
    "#\t probs: probabilities for the current row\n",
    "#\t means: mean of each cluster (list)\n",
    "#\t covs: covariance of each cluster (list)\n",
    "# Return: [row, [likelihood of k==0, likelihood of k==1, ...]]\n",
    "def map_LogLike(x, k, probs, means, covs):\n",
    "\t# Get log likelihoods for each k cluster\n",
    "\tresult = []\n",
    "\tlikelihood = 0\n",
    "\t# Compute the Likelihood (probability) for each cluster, k\n",
    "\tlog_l = np.array(Log_Likelihood(x, k, probs, means, covs))\n",
    "\tfor i in range(k):\n",
    "\t\ttry:\t# Could be zero\n",
    "\t\t\tlog_l[i] = log_l[i] - logsumexp(log_l)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\ttry:\t# Could be zero\n",
    "\t\tlikelihood = np.exp(log_l)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tresult.append(x[0])\n",
    "\tresult.append(likelihood)\n",
    "\treturn result\n",
    "\n",
    "# Compute the new gaussian parameters given the data and prior probabilities\n",
    "# Input: the_rdd: Data matrix of row attributes (RDD)\n",
    "#        k: number of clusters\n",
    "#        prob_mat: (N x k) probability matrix\n",
    "#        means: means of each cluster\n",
    "#        covs: Covariances of each cluster\n",
    "# Return: rdd_LL: [data_points, log_likelihoods]\n",
    "def Expectation(the_rdd, k, prob_mat, means, covs):\n",
    "\trdd_LL = the_rdd.map(lambda x : map_LogLike(x, k, prob_mat, means, covs))\n",
    "\treturn rdd_LL\n",
    "\n",
    "#Maximization Step Map/Reduce Functions\n",
    "\n",
    "# Reduce function to add the probabilities\n",
    "# Input: K: number of clusters\n",
    "#\t x: [0]: the data point (list)\n",
    "#\t    [1]: the probabilities (likelihoods) of [0] for each K (list)\n",
    "#\t y: [0]: the other data point (list)\n",
    "#\t    [1]: the probabilities (likelihoods) of [0] for each K (list)\n",
    "# Return: [0, [summed likelihoods (length k)]]\n",
    "def reduce_add_Prob(K, x, y):\n",
    "\treturn [0, [x[1][i] + y[1][i] for i in range(K)]]\n",
    "\n",
    "# Map function to compute new partial means for each k (probability times data point = expected value)\n",
    "# Input: K: number of clusters\n",
    "#\t x: [0]: the data point (list)\n",
    "#\t    [1]: the probabilities (likelihoods) of [0] for each K (list)\n",
    "# Return: [x[0], x[1], x[0]*[i] for i to k]\n",
    "def map_Means(K, x):\n",
    "\treturn x + [[x[0] * x[1][i] for i in range(K)]]\n",
    "\n",
    "# Reduce function to Add means for each K (Add expected values from different clusters together) \n",
    "# Input: K: number of clusters\n",
    "#\t x: output of map_Means (third element is a K * D 2d list)\n",
    "#\t y: output of map_Means (third element is a K * D 2d list)\n",
    "# Return: [0, 0, [summed expected values for each k (k seperate lists of length D)]]\n",
    "def reduce_Means(K, x, y):\n",
    "\treturn [0,0,[x[2][i] + y[2][i] for i in range(K)]]\n",
    "\n",
    "# Map function to compute partial Covariances for each cluster\n",
    "# Input: K: number of clusters\n",
    "#\t x: output of map_Means (third element is a K * D 2d list)\n",
    "#\t means: List of k mean arrays, all of size D\n",
    "# Return: [output of map_Means, partial Covariance Matrix for each K cluster times N * probability of k]\n",
    "def map_Covs(K, x, means):\n",
    "\treturn x + [[x[1][i]*np.outer(x[0]-means[i],x[0]-means[i]) for i in range(K)]]\n",
    "\n",
    "# Reduce function to add up the partial covariance matrices for each cluster\n",
    "# Input: K: number of clusters\n",
    "#\t x: output of map_Covs\n",
    "#\t y: output of map_Covs\n",
    "# Return: [0, 0, 0, [D x D covariance matrix for each k (times N times probability of k)]]\n",
    "def reduce_Covs(K, x, y):\n",
    "\treturn [0,0,0,[x[3][i] + y[3][i] for i in range(K)]]\n",
    "\n",
    "def Maximization(the_rdd, k, N):\n",
    "\t# Compute Probabilities\n",
    "\tNprob_mat = np.array(the_rdd.reduce(lambda x, y : reduce_add_Prob(k, x, y))[1]).clip(1e-10,np.inf)\n",
    "\tprob_mat = Nprob_mat / N\n",
    "\n",
    "\t# Compute Means \n",
    "\trdd_means = the_rdd.map(lambda x : map_Means(k, x))\n",
    "\tmeans_t = rdd_means.reduce(lambda x, y : reduce_Means(k, x, y))[2]\n",
    "\tmeans = [means_t[k]/Nprob_mat[k] for k in range(k)]\t# Divide by N and the expected values to yeild the mean for each dimension, per k cluster\n",
    "\n",
    "\t# Compute Covariance Matrices\n",
    "\trdd_covs = rdd_means.map(lambda x: map_Covs(k, x, means))\n",
    "\tcovs_t = rdd_covs.reduce(lambda x, y : reduce_Covs(k, x, y))[3]\n",
    "\tcovs = [covs_t[k]/Nprob_mat[k] for k in range(k)]\t# Divide by N and the likelihoods for the correct covariance matrices\n",
    "\treturn prob_mat, means, covs\n",
    "\n",
    "# Compute the Sum of the Log Likelihoods the_rdd\n",
    "# Input: the_rdd: The RDD of the data (N x D matrix)\n",
    "#\t k: Number of clusters\n",
    "#\t probs: Probability of each row belonging to each k cluster\n",
    "#\t\t(N x k)\n",
    "#\t means: Means of each cluster\n",
    "def lgsumexp_ll(the_rdd, k, probs, means, covs):\n",
    "\treturn the_rdd.map(lambda x: logsumexp(np.array(Log_Likelihood(x, k, probs, means, covs)))).reduce(lambda x,y: x+y)\n",
    "\n",
    "# Predicts the Cluster for each row of the RDD\n",
    "# Input: the_rdd: the RDD of the data (N x D matrix)\n",
    "#\t k: Number of clusters\n",
    "#\t N: Number of observations\n",
    "#\t probs: Probability of each row belonging to each k cluster\n",
    "#\t\t(N x k)\n",
    "#\t means: Means of each cluster\n",
    "#\t covs: Covariances of each cluster\n",
    "# Returns: Numpy array of predictions for each row\n",
    "def EM_predict_cluster(the_rdd, k, N, probs, means, covs):        \n",
    "\treturn np.array(the_rdd.map(lambda x: np.array(Log_Likelihood(x, k, probs, means, covs))).map(lambda x: np.argmax(x)).take(N))\n",
    "\n",
    "# Helper function for getPerformanceMetrics.\n",
    "# Computes number of correct/incorrect predictions,\n",
    "# as well as true positives/negatives, and false positives/negatives\n",
    "# Input: validData: the valid list of categories\n",
    "#\t predictions: the predicted list of categories\n",
    "# Return: number of correct, incorrect, true positives,\n",
    "#\t  true negatives, false positives, and false negatives\n",
    "def getPerformanceStatistics(validData, predictions):\n",
    "\tcorrect = 0\n",
    "\tincorrect = 0\n",
    "\tfNeg = 0\n",
    "\tfPos = 0\n",
    "\ttPos = 0\n",
    "\ttNeg = 0\n",
    "\tfor x in range(len(validData)):\n",
    "\t\tif(validData[x] == predictions[x]):\n",
    "\t\t\tcorrect += 1\n",
    "\t\t\tif(validData[x] == 0):\t\t\t\t#True negative\n",
    "\t\t\t\ttNeg +=1\n",
    "\t\t\telse:\t\t\t\t\t\t#True positive\n",
    "\t\t\t\ttPos +=1\n",
    "\t\telse:\n",
    "\t\t\tincorrect += 1\n",
    "\t\t\t# 1 == seizure (positive), 0 == no seizure (negative)\n",
    "\t\t\tif(validData[x] == 1):\t\t\t\t#False negative\n",
    "\t\t\t\tfNeg +=1\n",
    "\t\t\telse:\t\t\t\t\t\t#False positive\n",
    "\t\t\t\tfPos +=1\n",
    "\treturn correct, incorrect, tPos, tNeg, fPos, fNeg\n",
    "\n",
    "#Report performance metrics from the validation dataset and list of predictions\n",
    "#Returns tuple\n",
    "def getPerformanceMetrics(validData, predictions):\n",
    "\tP, N, TP, TN, FP, FN = getPerformanceStatistics(validData, predictions)\n",
    "\taccuracy = (P / (P + N)) * 100.0\n",
    "\tif((TP + FN) == 0):\n",
    "\t\tTPR = np.inf\n",
    "\telse:\n",
    "\t\tTPR = TP / (TP + FN) * 100.0\t#True positive rate; recall; sensitivity\n",
    "\tif((TP + FP) == 0):\n",
    "\t\tPPV = np.inf\n",
    "\telse:\n",
    "\t\tPPV = TP / (TP + FP) * 100.0\t#Positive predictive value; precision\n",
    "\tif((TN + FP) == 0):\n",
    "\t\tTNR = np.inf\n",
    "\telse:\n",
    "\t\tTNR = TN / (TN + FP) * 100.0\t#True negative rate; specificity\n",
    "\tif((PPV + TPR) == 0):\n",
    "\t\tF = np.inf\n",
    "\telse:\n",
    "\t\tF = 2 * PPV * TPR / (PPV + TPR)\t#F1 Score\n",
    "\treturn (P, N, TP, TN, FP, FN, accuracy, TPR, PPV, TNR, F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Expectation Maximization on Training Data for  2 clusters\n",
      "Iteration: 1 Maximum Change in Centroids: 6.4834010067322705 ; Change in Sum of Log-Likelihoods: 715541.4809366337\n",
      "Iteration: 2 Maximum Change in Centroids: 2.0539911895102705 ; Change in Sum of Log-Likelihoods: 255871.61114855914\n",
      "Iteration: 3 Maximum Change in Centroids: 0.046655395161797335 ; Change in Sum of Log-Likelihoods: 188261.1580830281\n",
      "Iteration: 4 Maximum Change in Centroids: 0.01161510962181871 ; Change in Sum of Log-Likelihoods: 255792.82374909212\n",
      "Iteration: 5 Maximum Change in Centroids: 0.008320326580983582 ; Change in Sum of Log-Likelihoods: 188217.43539285893\n",
      "Iteration: 6 Maximum Change in Centroids: 0.007042456856634416 ; Change in Sum of Log-Likelihoods: 255782.55338817794\n",
      "Gaussian Expectation Maximization Converged after 6 iterations\n",
      "Silhouette index for training Dataset: 0.5002573590946864\n",
      "Training Set Performance Metrics:\n",
      "Accuracy: 96.48%\n",
      "True Positives: 1661\n",
      "True Negatives: 6829\n",
      "False Positives: 238\n",
      "False Negatives: 72\n",
      "True Positive Rate (sensitivity): 95.85%\n",
      "Positive Prediction Value (precision): 87.47%\n",
      "True Negative Rate (specificity): 96.63%\n",
      "F1 Score: 91.46%\n"
     ]
    }
   ],
   "source": [
    "    #Command Line Argument\n",
    "    k = 2\n",
    "    if k <= 0:\n",
    "        print(\"First arguement must be > 0\")\n",
    "        sys.quit()\n",
    "    print(\"Performing Expectation Maximization on Training Data for \", k, \"clusters\")\n",
    "\n",
    "    #Load data matrix\n",
    "    my_data = genfromtxt('input/train/TrainingData_Projected.txt', delimiter=',')\n",
    "    testing_data = genfromtxt('input/test/TestingData_Projected.txt', delimiter=',')\n",
    "\n",
    "    #Delete first row: header\n",
    "    my_data = np.delete(my_data, 0, 0)\n",
    "    #Delete first column: ID\n",
    "    IDs = my_data[:,0].copy()\n",
    "    IDs = IDs.reshape(np.size(my_data, 0),1)\n",
    "    my_data = np.delete(my_data, 0, 1)\n",
    "    #Collect Correct labels and delete label column (last column)\n",
    "    labels = my_data[:,-1].copy()\n",
    "    my_data = np.delete(my_data, -1, 1)\n",
    "\n",
    "    # Create RDD from data\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    points = sc.parallelize(my_data, numSlices=100)\n",
    "\n",
    "    # Find Centroids (kmeans++ method)\n",
    "    centroids, dist_matrix = centroid(my_data, k)\n",
    "    # Assign each point to the centroid closest to it\n",
    "    clustered_points = assignPointsToClosestCluster(points, centroids)\n",
    "    points_arr = clustered_points.collect().copy()\n",
    "\n",
    "    #Extract cluster labels\n",
    "    label = [i[0] for i in points_arr]\n",
    "    points_arr = [i[1] for i in points_arr]\n",
    "    points_arr = np.vstack(points_arr)\n",
    "    N = np.size(points_arr, 0)\n",
    "    dimension = np.size(points_arr, 1)\n",
    "    data = np.column_stack((points_arr, label))\n",
    "\n",
    "    data_arranged = []\n",
    "    pi_arr = np.zeros(k)\n",
    "    for i in list(set(label)):\n",
    "        data_arranged.append(data[np.logical_or.reduce([data[:,-1] == i])])\n",
    "    iter = 0\n",
    "\n",
    "    #Probabilities for each initial class\n",
    "    for i in label:\n",
    "        if ((data[:,-1] == i)[iter]):\n",
    "            pi_arr[i] += 1\n",
    "        iter += 1\n",
    "    pi_arr = pi_arr / np.size(label, 0)\n",
    "\n",
    "    #Covariances and mean for each initial class\n",
    "    cov_arr = []\n",
    "    mean_arr = []\n",
    "    arr = list(range(1,dimension+1))\n",
    "    for i in list(set(label)):\n",
    "        cov_arr.append((np.dot(data_arranged[i][:,arr].T, data_arranged[i][:,arr]) / N).squeeze())\n",
    "        mean_arr.append(np.mean(data_arranged[i][:,arr], 0))\n",
    "\n",
    "    #data_points = data[:,arr].reshape(N,dimension).copy()\n",
    "    #label = data[:,0].copy()\n",
    "    data_points = data[:,range(0,dimension)].reshape(N,dimension).copy()\n",
    "\n",
    "    # loop until parameters converge\n",
    "    data_list = points.map(lambda x: [x])\n",
    "    shift = sys.maxsize\n",
    "    old_shift = sys.maxsize\n",
    "    old_old_shift = sys.maxsize\n",
    "    dist_centers = sys.maxsize\n",
    "    epsilon = 0.005\n",
    "    iters = 0\n",
    "    ll_change = np.inf\n",
    "    while(shift > epsilon and ll_change > epsilon):\n",
    "        iters += 1\n",
    "\n",
    "        #Expectation Step\n",
    "        the_rdd = Expectation(data_list, k, pi_arr, mean_arr, cov_arr)\n",
    "\n",
    "        #Maximization Step\n",
    "        new_pi_arr, new_mean_arr, new_cov_arr = Maximization(the_rdd, k, N)\n",
    "\n",
    "        # Calculate difference of Log Likelihood\n",
    "        l = abs(lgsumexp_ll(data_list, k, pi_arr, mean_arr, cov_arr))\n",
    "        if (iters == 1):\n",
    "            ll_change = l\n",
    "        else:\n",
    "            ll_change = abs(l - ll_change)\n",
    "\n",
    "        #Calculate Shift (Maximum Shift in Centroids)\n",
    "        old_old_shift = old_shift\n",
    "        old_shift = shift\n",
    "        shift = calculateChangeInCentroids(mean_arr, new_mean_arr)\n",
    "\n",
    "        #Update Variables\n",
    "        mean_arr = new_mean_arr.copy()\n",
    "        cov_arr = new_cov_arr.copy()\n",
    "        pi_arr = new_pi_arr.copy()\n",
    "\n",
    "        #Log Iteration\n",
    "        print(\"Iteration:\", iters, \"Maximum Change in Centroids:\", shift, \"; Change in Sum of Log-Likelihoods:\", float(ll_change))\n",
    "\n",
    "        # Prevent oscillation\n",
    "        if(abs(old_old_shift - shift) <= epsilon):\n",
    "            break;\n",
    "\n",
    "    predicted_labels = EM_predict_cluster(the_rdd, k, N, pi_arr, mean_arr, cov_arr)        \n",
    "\n",
    "    print(\"Gaussian Expectation Maximization Converged after\", iters, \"iterations\")\n",
    "    if(not all(elem == predicted_labels[0] for elem in predicted_labels)):\n",
    "        sil_in = silhouette_score(data_points, predicted_labels)\n",
    "        print(\"Silhouette index for training Dataset:\", float(sil_in))\n",
    "    else:\n",
    "        print(\"Gaussian Expectation Maximization Failed\")\n",
    "\n",
    "    #Prepend Row numbers for Plotting the Data\n",
    "    new_data = np.matrix([])\n",
    "    new_data = np.hstack((predicted_labels.reshape(N, 1), data_points))\n",
    "\n",
    "    # Print metrics\n",
    "    P, N, TP, TN, FP, FN, accuracy, TPR, PPV, TNR, F = getPerformanceMetrics(labels, predicted_labels)\n",
    "    print('Training Set Performance Metrics:')\n",
    "    print('Accuracy: ' + \"{0:.2f}\".format(round(float(accuracy),2)) + '%')\n",
    "    print('True Positives: ' + str(TP))\n",
    "    print('True Negatives: ' + str(TN))\n",
    "    print('False Positives: ' + str(FP))\n",
    "    print('False Negatives: ' + str(FN))\n",
    "    print('True Positive Rate (sensitivity): ' + \"{0:.2f}\".format(round(float(TPR),2)) + '%')\n",
    "    print('Positive Prediction Value (precision): ' + \"{0:.2f}\".format(round(float(PPV),2)) + '%')\n",
    "    print('True Negative Rate (specificity): ' + \"{0:.2f}\".format(round(float(TNR),2)) + '%')\n",
    "    print('F1 Score: ' + \"{0:.2f}\".format(round(float(F),2)) + '%')\n",
    "\n",
    "    #Save output Files\n",
    "    filename = \"output/Data_EM_\" + str(k) + \".txt\"\n",
    "    np.savetxt(filename, new_data, delimiter=',', fmt=' '.join(['%d,'] + ['%d,'] + ['%1.4f,']*(np.size(new_data, axis=1)-3) + ['%1.4f']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Expectation Maximization on Testing Data for  2 clusters\n",
      "Silhouette index for testing Dataset: 0.37820892916707155\n",
      "Testing Set Performance Metrics:\n",
      "Accuracy: 87.18%\n",
      "True Positives: 459\n",
      "True Negatives: 1894\n",
      "False Positives: 240\n",
      "False Negatives: 106\n",
      "True Positive Rate (sensitivity): 81.24%\n",
      "Positive Prediction Value (precision): 65.67%\n",
      "True Negative Rate (specificity): 88.75%\n",
      "F1 Score: 72.63%\n"
     ]
    }
   ],
   "source": [
    "    print(\"Performing Expectation Maximization on Testing Data for \", k, \"clusters\")\n",
    "    #Delete first row: header\n",
    "    testing_data = np.delete(testing_data, 0, 0)\n",
    "    #Delete first column: ID\n",
    "    testing_data = np.delete(testing_data, 0, 1)\n",
    "    #Collect labels and delete label column (last column)\n",
    "    labels = testing_data[:,-1].copy()\n",
    "    testing_data = np.delete(testing_data, -1, 1)\n",
    "\n",
    "    # Create RDD from data\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    test_points = sc.parallelize(testing_data, numSlices=50)\n",
    "    N = np.size(testing_data, 0)\n",
    "\n",
    "    # Assign each point to the centroid closest to it\n",
    "    predicted_labels = EM_predict_cluster(test_points, k, N, pi_arr, mean_arr, cov_arr)\n",
    "    sc.stop()\n",
    "\n",
    "    sil_in = silhouette_score(testing_data, predicted_labels)\n",
    "    print(\"Silhouette index for testing Dataset:\", float(sil_in))\n",
    "\n",
    "    #Prepend Categories for Plotting the Data\n",
    "    N = np.size(testing_data, 0)\n",
    "    predicted_labels = np.array(predicted_labels).reshape(N, 1)\n",
    "    testing_data = np.hstack((predicted_labels, testing_data))\n",
    "\n",
    "    # Print metrics\n",
    "    P, N, TP, TN, FP, FN, accuracy, TPR, PPV, TNR, F = getPerformanceMetrics(labels, predicted_labels)\n",
    "    print('Testing Set Performance Metrics:')\n",
    "    print('Accuracy: ' + \"{0:.2f}\".format(round(float(accuracy),2)) + '%')\n",
    "    print('True Positives: ' + str(TP))\n",
    "    print('True Negatives: ' + str(TN))\n",
    "    print('False Positives: ' + str(FP))\n",
    "    print('False Negatives: ' + str(FN))\n",
    "    print('True Positive Rate (sensitivity): ' + \"{0:.2f}\".format(round(float(TPR),2)) + '%')\n",
    "    print('Positive Prediction Value (precision): ' + \"{0:.2f}\".format(round(float(PPV),2)) + '%')\n",
    "    print('True Negative Rate (specificity): ' + \"{0:.2f}\".format(round(float(TNR),2)) + '%')\n",
    "    print('F1 Score: ' + \"{0:.2f}\".format(round(float(F),2)) + '%')\n",
    "\n",
    "    #Save output Files\n",
    "    filename = \"output/TestingData_EM_\" + str(k) + \".txt\"\n",
    "    np.savetxt(filename, testing_data, delimiter=',', fmt='%3.4f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
