{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Data onto first 40 Principal Components and split into Testing/Training Datasets. See file data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\"\"\"\n",
    "Name: Dustin Mcafee\n",
    "Standardize and Project data to Principal Components\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#This name is misleading: This doesn't load data, it splits data.\n",
    "def loadData(dataset, split):\n",
    "\t#Copy the data as to not randomize the original set\n",
    "\tdata = dataset.copy()\n",
    "\tnp.random.shuffle(data)\n",
    "\n",
    "\t#Split the randomized dataset into a training dataset and a testing dataset\n",
    "\tvalid, train = data[:split,:], data[split:,:]\n",
    "\treturn train, valid\n",
    "\n",
    "#This function actually loads the data\n",
    "def loadData2(filename):\n",
    "\tif(os.path.exists(filename)):\n",
    "\t\tdata = np.genfromtxt(filename, delimiter=',')\n",
    "\telse:\n",
    "\t\tprint(\"Second argument must be a valid input Training Dataset (numerical, csv) file with header row, first column is index column, and last column categorical (discrete-numerical)\")\n",
    "\t\tsys.exit()\n",
    "\treturn data\n",
    "\n",
    "#Clean the data (Impute nan rows)\n",
    "def imputeNAN(data, array):\n",
    "\trow_it = 0\n",
    "\tfor row in data:\n",
    "\t\tcol_it = 0\n",
    "\t\tnum_imput = 0\n",
    "\t\tfor elem in row:\n",
    "\t\t\tif(np.isnan(elem)):\n",
    "\t\t\t\tdata[row_it, col_it] = array[col_it]\n",
    "\t\t\t\tnum_imput = num_imput + 1\n",
    "\t\t\tcol_it = col_it + 1\n",
    "\t\tif(num_imput > 0):\n",
    "\t\t\tprint(num_imput, \"Imputed in row\", row_it)\n",
    "\t\trow_it = row_it + 1\n",
    "\treturn data\n",
    "\n",
    "#Standardize data\n",
    "def standardize(data, mean):\n",
    "\t#Variables\n",
    "\tN = np.size(data, 0)\n",
    "\tdimensions = np.size(data, 1)\n",
    "\n",
    "\t# Center data around sample mean\n",
    "\tcenter_x = data.copy()\n",
    "\tcenter_x -= mean\n",
    "\n",
    "\t# Covariance matrix\n",
    "\tcov = (np.dot(center_x.T, center_x.conj()) / N).squeeze()\n",
    "\n",
    "\t# Standardize Data (z-normalize)\n",
    "\tD = np.sqrt(np.diag(cov))\n",
    "\ts = (dimensions, dimensions)\n",
    "\tdev = np.zeros(s)    #Standard Deviation Values\n",
    "\ti = 0\n",
    "\tfor elem in D:\n",
    "\t\tdev[i,i] = elem\n",
    "\t\ti = i + 1\n",
    "\n",
    "\tDInv = np.linalg.inv(dev)\n",
    "\tstandard_data = np.matmul(center_x, DInv).astype(float)\n",
    "\n",
    "\treturn (cov, standard_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 Dimensions\n",
      "11500 Observations\n",
      "First 40 PC covers 95.87264459229334 percent of the Variance\n"
     ]
    }
   ],
   "source": [
    "\tPC = 40\n",
    "\tif(PC < 1):\n",
    "\t\tprint(\"First input argument must be the number of Principal Components to project data onto\")\n",
    "\t\tsys.quit()\n",
    "\n",
    "\tmy_data = loadData2('input/data.csv')\n",
    "\n",
    "\t#Delete first column: ID\n",
    "\tmy_data = np.delete(my_data, 0, 1)\n",
    "\t#Delete header row\n",
    "\tmy_data = np.delete(my_data, 0, 0)\n",
    "\n",
    "\t#Collect labels and delete label column (last column)\n",
    "\tdimensions = np.size(my_data, 1)\n",
    "\tlabels = my_data[:,-1].copy()\n",
    "\tmy_data = np.delete(my_data, -1, 1)\n",
    "\tfor i in range(len(labels)):\n",
    "\t\tif(not labels[i] == 1):\n",
    "\t\t\tlabels[i] = 0\n",
    "\n",
    "\t#Variables\n",
    "\tN = np.size(my_data, 0)\n",
    "\tlabels = labels.reshape(N,1)\n",
    "\tdimensions = np.size(my_data, 1)\n",
    "\tprint(dimensions, \"Dimensions\")\n",
    "\tprint(N, \"Observations\")\n",
    "\tarr = np.array([])\n",
    "\tfor i in range(N):\n",
    "\t\tarr = np.append(arr, i)\n",
    "\tarr = arr.reshape(N,1)\n",
    "\n",
    "\t#impute missing data\n",
    "\tmean = np.nanmean(my_data, axis=0)\n",
    "\tmy_data = imputeNAN(my_data, mean)\n",
    "\n",
    "\t#Standardize (Z-Normalize) the Data\n",
    "\tcov, standard_data = standardize(my_data, mean)\n",
    "\tmy_data = standard_data.copy()\n",
    "\n",
    "\t#Singular Value Decomposition\n",
    "\tu, s, vh = np.linalg.svd(my_data, full_matrices=True)\n",
    "\tss = np.square(s)\n",
    "\n",
    "\t#Percentage of Variance for each k PC\n",
    "\tss_sum = np.sum(ss)\n",
    "\tss_percent = np.divide(ss,ss_sum) * 100\n",
    "\tsum = 0\n",
    "\tfor i in range(0, PC):\n",
    "\t\tsum = sum + ss_percent[i]\n",
    "\t\t\n",
    "\tprint(\"First\", PC, \"PC covers\", sum, \"percent of the Variance\")\n",
    "\n",
    "\t#Project to first PCs\n",
    "\tv = np.transpose(vh)\n",
    "\trang = range(0,PC)\n",
    "\tv_pc = v[:, rang]\n",
    "\tdata_pc = np.matmul(my_data, v_pc)\n",
    "\n",
    "\t#Save output Files: Header row (numbered), First column ID, Last column Category Label\n",
    "\thead = np.array([])\n",
    "\tfor i in range(dimensions+2):\n",
    "\t\tj = i\n",
    "\t\tif(j > 0):\n",
    "\t\t\tj = j - 1\n",
    "\t\thead = np.append(head, j)\n",
    "\thead = head.reshape(1,dimensions+2)\n",
    "\tdat_stand = np.vstack((head, np.hstack((arr, np.hstack((my_data, labels))))))\n",
    "\tnp.savetxt(\"output/Data_Standardized.txt\", dat_stand, delimiter=',', fmt='%3.4f') \n",
    "\tnp.savetxt(\"output/Singular_Values.txt\", s, delimiter=',', fmt='%3.4f')\n",
    "\tnp.savetxt(\"output/Singular_Values_Percent_Variance.txt\", ss_percent, delimiter=',', fmt='%3.4f')\n",
    "\thead = np.array([])\n",
    "\tfor i in range(PC+2):\n",
    "\t\tj = i\n",
    "\t\tif(j > 0):\n",
    "\t\t\tj = j - 1\n",
    "\t\thead = np.append(head, j)\n",
    "\thead = head.reshape(1,PC+2)\n",
    "\tdat_proj = np.vstack((head, np.hstack((arr, np.hstack((data_pc, labels))))))\n",
    "\tnp.savetxt(\"output/Data_Project.txt\", dat_proj, delimiter=',', fmt='%3.4f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING: DO NOT RUN LAST CODE BLOCK UNLESS YOU WISH TO GENERATE NEW TESTING/TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Dimensions\n",
      "8801 Triaining Set Observations\n",
      "2700 Test Set Observations\n"
     ]
    }
   ],
   "source": [
    "\t# Generate training/testing data\n",
    "\ttrainData, testData = loadData(dat_proj, 2700)\n",
    "\t# Print Dataset sizes\n",
    "\tprint(np.size(trainData, 1) - 2, \"Dimensions\")\n",
    "\tprint(np.size(trainData, 0), \"Triaining Set Observations\")\n",
    "\tprint(np.size(testData, 0), \"Test Set Observations\")\n",
    "\tnp.savetxt(\"input/train/TrainingData_Projected.txt\", np.matrix(trainData), delimiter=',', fmt=' '.join(['%3.4f,'] * (np.size(trainData, 1) - 1) + ['%d']))\n",
    "\tnp.savetxt(\"input/test/TestingData_Projected.txt\", np.matrix(testData), delimiter=',', fmt=' '.join(['%3.4f,'] * (np.size(testData, 1) - 1) + ['%d']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
